

# concept
### PCIe交换机芯片：
在高性能GPU计算的领域内，关键组件如CPU、内存模块、NVMe存储设备、GPU以及网络适配器等通过PCIe（外设部件互连标准）总线或专门设计的PCIe交换机芯片实现高效顺畅的连接。历经五代技术革新，目前最新的Gen5版本确保了设备间极为高效的互连性能。这一持续演进充分彰显了PCIe在构建高性能计算系统中的核心地位，显著提升了数据传输速度，并有力地促进了现代计算集群中各互联设备间的无缝协同工作。

### NVLink定义：
NVLink是英伟达（NVIDIA）开发并推出的一种总线及其通信协议。NVLink采用点对点结构、串列传输，用于中央处理器（CPU）与图形处理器（GPU）之间的连接，也可用于多个图形处理器之间的相互连接。与PCI Express不同，一个设备可以包含多个NVLink，并且设备之间采用网格网络而非中心集线器方式进行通信。该协议于2014年3月首次发布，采用专有的高速信号互连技术（NVHS）。
NVLink的发展历程：从NVLink 1.0到NVLink 4.0
NVLink 1.0
连接方式：采用4通道连接。
总带宽：实现高达160 GB/s的双向总带宽。
用途：主要用于加速GPU之间的数据传输，提升协同计算性能。
NVLink 2.0
连接方式：基于6通道连接。
总带宽：将双向总带宽提升至300 GB/s。
性能提升：提供更高的数据传输速率，改善GPU间通信效率。
NVLink 3.0
连接方式：采用12通道连接。
总带宽：达到双向总带宽600 GB/s。
新增特性：引入新技术和协议，提高通信带宽和效率。
NVLink 4.0
连接方式：使用18通道连接。
总带宽：进一步增加至双向总带宽900 GB/s。
性能改进：通过增加通道数量，NVLink 4.0能更好地满足高性能计算和人工智能应用对更大带宽的需求。
NVLink 1.0、2.0、3.0和4.0之间的关键区别主要在于**连接通道数目的增加、所支持的总带宽以及由此带来的性能改进**。随着版本迭代，NVLink不断优化GPU间的数据传输能力，以适应日益复杂且要求严苛的应用场景。

### NVSwitch：
NVSwitch是NVIDIA专为满足高性能计算和人工智能应用需求而研发的一款交换芯片，其核心作用在于**实现同一主机内部多颗GPU之间的高速、低延迟通信**。


### NVLink交换机：
NVLink交换机是一种由NVIDIA专为在分布式计算环境中的**不同主机间实现GPU设备间高性能通信而设计制造的独立交换设备**。不同于集成于单个主机内部GPU模块上的NVSwitch，NVLink交换机旨在解决跨主机连接问题。可能有人会混淆NVLink交换机和NVSwitch的概念，但实际上早期提及的“NVLink交换机”是指安装在GPU模块上的切换芯片。直至2022年，NVIDIA将此芯片技术发展为一款独立型交换机产品，并正式命名为NVLink交换机。

### HBM（高带宽内存）
传统上，GPU内存与常见的DDR（双倍数据速率）内存相似，通过物理插槽插入主板并通过PCIe接口与CPU或GPU进行连接。然而，这种配置在PCIe总线中造成了带宽瓶颈，其中Gen4版本提供64GB/s的带宽，Gen5版本则将其提升至128GB/s。
为了突破这一限制，包括但不限于NVIDIA在内的多家GPU制造商采取了创新手段，即将多个DDR芯片堆叠整合，形成了所谓的高带宽内存（HBM）。例如，在探讨H100时所展现的设计，GPU直接与其搭载的HBM内存相连，无需再经过PCIe交换芯片，从而极大地提高了数据传输速度，理论上可实现显著的数量级性能提升。因此，“高带宽内存”（HBM）这一术语精准地描述了这种先进的内存架构。

### 带宽单位解析
在网络通信场景下，数据速率通常以每秒比特数（bit/s）表示，且为了区分发送（TX）和接收（RX），常采用单向传输速率来衡量。而在诸如PCIe、内存、NVLink及HBM等其他硬件组件中，带宽指标则通常使用每秒字节数（Byte/s）或每秒事务数（T/s）来衡量，并且这些测量值一般代表双向总的带宽容量，涵盖了上行和下行两个方向的数据流。
